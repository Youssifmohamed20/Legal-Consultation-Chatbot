# -*- coding: utf-8 -*-
"""Yousif_Mohamed_Yousif_Elgendy_Law_NTI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g5ZE3mbPC1iBM1q8bMK4nMfL8Tc2MJ8h
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df = pd.read_csv("/content/drive/MyDrive/Youssif/Legal data.csv")

df.head()

df.isna().sum()

df.dropna(subset=["question", "answer"], inplace=True)

df.isna().sum()

import re

def arabic_clean(text):
    text = str(text)
    text = re.sub(r'[\u0617-\u061A\u064B-\u0652]', '', text)
    text = re.sub(r'[Ù‘ÙÙ‹ÙÙŒÙÙÙ’Ù€]', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'(.)\1{2,}', r'\1', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

df['question'] = df['question'].apply(arabic_clean)
df['answer'] = df['answer'].apply(arabic_clean)

!pip install -q faiss-cpu

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import numpy as np
import faiss

retriever = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
questions = df['question'].tolist()
embeddings = retriever.encode(questions, show_progress_bar=True)
index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(np.array(embeddings))

def retrieve_answer(query):

    query_clean = query.strip()


    matches = df[df['question'] == query_clean]

    if matches.empty:
        return None
    else:
        return matches.iloc[0]['answer']

from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("aubmindlab/aragpt2-base")
model = AutoModelForCausalLM.from_pretrained("aubmindlab/aragpt2-base")

def generate_simplified_answer(answer, max_length=120):

    prompt = f"""\
Ø¯ÙŠ Ø¥Ø¬Ø§Ø¨Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø±Ø³Ù…ÙŠØ©:
{answer}

Ø¹Ø§ÙˆØ²Ùƒ ØªØ´Ø±Ø­Ù„ÙŠ Ø¨Ù„ØºØ© Ø³Ù‡Ù„Ø© ÙƒØ£Ù†ÙŠ ÙˆØ§Ø­Ø¯ Ù…Ø´ ÙØ§Ù‡Ù… ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø®Ø§Ù„ØµØŒ ÙˆØ¨Ø§Ù„Ø¹Ø§Ù…ÙŠØ© Ø§Ù„Ù…ØµØ±ÙŠØ© Ù„Ùˆ ÙŠÙ†ÙØ¹:
"""


    inputs = tokenizer.encode(prompt.strip(), return_tensors="pt", truncation=True, max_length=512)


    outputs = model.generate(
        inputs,
        max_length=max_length,
        do_sample=True,
        top_k=40,
        top_p=0.95,
        temperature=0.2,
        num_return_sequences=1,
        pad_token_id=tokenizer.eos_token_id
    )


    result = tokenizer.decode(outputs[0], skip_special_tokens=True)


    if "Ø¹Ø§ÙˆØ²Ùƒ" in result:
        result = result.split("Ø¹Ø§ÙˆØ²Ùƒ")[0].strip()
    elif "Ø¨Ø§Ù„Ø¹Ø§Ù…ÙŠØ©" in result:
        result = result.split("Ø¨Ø§Ù„Ø¹Ø§Ù…ÙŠØ©")[0].strip()

    return result.strip()

!pip install ddgs

from ddgs import DDGS

def search_web(query, max_results=3):
    try:
        with DDGS() as ddgs:
            results = ddgs.text(query, region='eg-en', safesearch='Moderate', max_results=max_results)
            return [result['body'] for result in results if 'body' in result]
    except Exception as e:
        print("âŒ Error during search:", e)
        return []

print(search_web("Ù…Ø§ Ù‡Ùˆ Ø§Ù„Ø·Ø¹Ù† Ø¨Ø§Ù„Ù†Ù‚Ø¶ØŸ"))

print(search_web("ÙƒÙ… Ø¹Ø¯Ø¯ Ø³Ù†ÙŠÙ† Ø­ÙƒÙ… Ø§Ù„Ù…Ø¤Ø¨Ø¯ "))

q = "ÙƒØ§Ù… Ø³Ù†Ø© Ø§Ù„Ù…Ø¤Ø¨Ø¯"
orig = retrieve_answer(q)

if orig is None:
    print("â— Ø§Ù„Ø³Ø¤Ø§Ù„ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©.")
else:
    simp = generate_simplified_answer(orig)
    print("Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©:", orig)
    print("Ø§Ù„ØªØ¨Ø³ÙŠØ· Ø§Ù„Ù…Ù‚ØªØ±Ø­:", simp)

def retrieve_answer(query):
    query_clean = query.strip()
    matches = df[df['question'] == query_clean]
    if matches.empty:
        return None
    else:
        return matches.iloc[0]['answer']


q = "Ø¹Ù„Ù‰ Ù…Ù† ØªØ±ÙØ¹ Ø§Ù„Ø¯Ø¹ÙˆÙ‰ Ø§Ù„Ø¬Ø²Ø§Ø¦ÙŠØ© Ø§Ù„Ø®Ø§ØµØ©ØŸ"
orig = retrieve_answer(q)

if orig is None:
    print("â— Ø§Ù„Ø³Ø¤Ø§Ù„ ØºÙŠØ± Ù…ÙˆØ¬ÙˆØ¯ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠØ©.")
else:
    simp = generate_simplified_answer(orig)
    print("Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ©:", orig)
    print("Ø§Ù„ØªØ¨Ø³ÙŠØ· Ø§Ù„Ù…Ù‚ØªØ±Ø­:", simp)

!pip install -q streamlit pyngrok faiss-cpu sentence-transformers transformers accelerate torch ddgs scikit-learn

# Commented out IPython magic to ensure Python compatibility.
# %%writefile yousif.py
# import streamlit as st
# import pandas as pd, re, os
# 
# 
# st.set_page_config(page_title="Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ù…Ø¨Ø³Ù‘Ø·", page_icon="âš–ï¸", layout="centered")
# st.markdown("<h1 style='text-align:center'>âš–ï¸ Ø§Ù„Ù…Ø³Ø§Ø¹Ø¯ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ø§Ù„Ù…Ø¨Ø³Ù‘Ø·</h1>", unsafe_allow_html=True)
# st.write("Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ ÙˆØ³Ù†ÙØ±Ø¬Ø¹ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø±Ø³Ù…ÙŠØ© Ù…Ù† Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø«Ù… ØªØ¨Ø³ÙŠØ·Ù‡Ø§ Ø¨Ø§Ù„Ø¹Ø§Ù…ÙŠØ©. Ø¥Ø°Ø§ Ù„Ù… Ù†Ø¬Ø¯Ù‡Ø§ ÙÙŠ Ø§Ù„Ù‚Ø§Ø¹Ø¯Ø©ØŒ Ù†Ø¹Ø±Ø¶ Ù…Ù„Ø®ØµØ§Øª Ù…Ù† Ø§Ù„ÙˆÙŠØ¨.")
# 
# 
# CSV_PATH = "/content/drive/MyDrive/Youssif/Legal data.csv"
# 
# 
# def arabic_clean(text):
#     text = str(text)
#     text = re.sub(r'[\u0617-\u061A\u064B-\u0652]', '', text)
#     text = re.sub(r'[Ù‘ÙÙ‹ÙÙŒÙÙÙ’Ù€]', '', text)
#     text = re.sub(r'[^\w\s]', '', text)
#     text = re.sub(r'\d+', '', text)
#     text = re.sub(r'(.)\1{2,}', r'\1', text)
#     text = re.sub(r'\s+', ' ', text)
#     return text.strip()
# 
# 
# def load_df(path):
#     if not os.path.exists(path):
#         return None, "Ù„Ø§ ÙŠÙ…ÙƒÙ† Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ Ø§Ù„Ù…Ø³Ø§Ø± Ø§Ù„Ù…Ø­Ø¯Ø¯."
#     try:
#         df = pd.read_csv(path)
#         if not {"question","answer"}.issubset(df.columns):
#             return None, "Ø§Ù„Ù…Ù„Ù Ù„Ø§ ÙŠØ­ØªÙˆÙŠ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© (question, answer)."
#         df = df.dropna(subset=["question","answer"]).copy()
#         df["question"] = df["question"].apply(arabic_clean)
#         df["answer"]   = df["answer"].apply(arabic_clean)
#         return df, None
#     except Exception:
#         return None, "ØªØ¹Ø°Ø± Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù„Ù. ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„ØªØ±Ù…ÙŠØ²/Ø§Ù„ØªÙ†Ø³ÙŠÙ‚."
# 
# df, df_err = load_df(CSV_PATH)
# 
# 
# def retrieve_answer(query: str):
#     if df is None:
#         return None
#     q = arabic_clean(query)
#     m = df[df["question"] == q]
#     return None if m.empty else m.iloc[0]["answer"]
# 
# 
# from transformers import AutoTokenizer, AutoModelForCausalLM
# tokenizer = AutoTokenizer.from_pretrained("aubmindlab/aragpt2-base")
# model     = AutoModelForCausalLM.from_pretrained("aubmindlab/aragpt2-base")
# 
# def generate_simplified_answer(answer, max_length=256):
#     prompt = f"""\
# Ø¯ÙŠ Ø¥Ø¬Ø§Ø¨Ø© Ù‚Ø§Ù†ÙˆÙ†ÙŠØ© Ø±Ø³Ù…ÙŠØ©:
# {answer}
# 
# Ø¹Ø§ÙˆØ²Ùƒ ØªØ´Ø±Ø­Ù„ÙŠ Ø¨Ù„ØºØ© Ø³Ù‡Ù„Ø© ÙƒØ£Ù†ÙŠ ÙˆØ§Ø­Ø¯ Ù…Ø´ ÙØ§Ù‡Ù… ÙÙŠ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ† Ø®Ø§Ù„ØµØŒ ÙˆØ¨Ø§Ù„Ø¹Ø§Ù…ÙŠØ© Ø§Ù„Ù…ØµØ±ÙŠØ© Ù„Ùˆ ÙŠÙ†ÙØ¹:
# """
#     inputs = tokenizer.encode(prompt.strip(), return_tensors="pt", truncation=True, max_length=512)
#     outputs = model.generate(
#         inputs, max_length=max_length, do_sample=True,
#         top_k=40, top_p=0.95, temperature=0.2,
#         num_return_sequences=1, pad_token_id=tokenizer.eos_token_id
#     )
#     result = tokenizer.decode(outputs[0], skip_special_tokens=True)
#     if "Ø¹Ø§ÙˆØ²Ùƒ" in result:   result = result.split("Ø¹Ø§ÙˆØ²Ùƒ")[0].strip()
#     elif "Ø¨Ø§Ù„Ø¹Ø§Ù…ÙŠØ©" in result: result = result.split("Ø¨Ø§Ù„Ø¹Ø§Ù…ÙŠØ©")[0].strip()
#     return result.strip()
# 
# 
# from ddgs import DDGS
# def search_web(query, max_results=3):
#     try:
#         with DDGS() as d:
#             res = d.text(query, region="eg-ar", safesearch="Moderate", max_results=max_results)
#             return [r.get("body","") for r in res if r.get("body")]
#     except Exception:
#         return []
# 
# 
# user_q = st.text_area("âœï¸ Ø§ÙƒØªØ¨ Ø³Ø¤Ø§Ù„Ùƒ Ø§Ù„Ù‚Ø§Ù†ÙˆÙ†ÙŠ Ù‡Ù†Ø§:", value="", height=120, placeholder="")
# 
# if df_err:
#     st.error(df_err)
# 
# if st.button("ğŸ” Ø¨Ø­Ø«") and user_q.strip():
#     if df_err:
# 
#         web = search_web(user_q, max_results=3)
#         if web:
#             st.subheader("ğŸŒ Ù…Ù„Ø®ØµØ§Øª Ù…Ù† Ø§Ù„ÙˆÙŠØ¨:")
#             for i, w in enumerate(web, 1):
#                 st.markdown(f"**Ù†ØªÙŠØ¬Ø© {i}:** {w}")
#         else:
#             st.warning("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù…Ù†Ø§Ø³Ø¨Ø© Ù…Ù† Ø§Ù„ÙˆÙŠØ¨ Ø§Ù„Ø¢Ù†.")
#     else:
# 
#         ans = retrieve_answer(user_q)
#         if ans:
#             st.success("âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø¥Ø¬Ø§Ø¨Ø© ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.")
#             st.subheader("ğŸ“„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø§Ù„Ø±Ø³Ù…ÙŠØ©:")
#             st.write(ans)
# 
#             st.subheader("ğŸ’¬ Ø§Ù„Ø´Ø±Ø­ Ø§Ù„Ù…Ø¨Ø³Ù‘Ø· (Ø¹Ø§Ù…ÙŠØ© Ù…ØµØ±ÙŠØ©):")
#             simp = generate_simplified_answer(ans, max_length=256)
#             st.write(simp if simp else "â€”")
#         else:
# 
#             st.warning("Ù„Ù… Ù†Ø¬Ø¯ Ø§Ù„Ø³Ø¤Ø§Ù„ ÙÙŠ Ù‚Ø§Ø¹Ø¯Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª. Ù†Ø¹Ø±Ø¶ Ù†ØªØ§Ø¦Ø¬ Ù…Ù† Ø§Ù„ÙˆÙŠØ¨:")
#             web = search_web(user_q, max_results=3)
#             if web:
#                 st.subheader("ğŸŒ Ù…Ù„Ø®ØµØ§Øª Ù…Ù† Ø§Ù„ÙˆÙŠØ¨:")
#                 for i, w in enumerate(web, 1):
#                     st.markdown(f"**Ù†ØªÙŠØ¬Ø© {i}:** {w}")
#             else:
#                 st.info("Ù„Ø§ ØªÙˆØ¬Ø¯ Ù†ØªØ§Ø¦Ø¬ Ù…Ù†Ø§Ø³Ø¨Ø© Ù…Ù† Ø§Ù„ÙˆÙŠØ¨ Ø§Ù„Ø¢Ù†. Ø­Ø§ÙˆÙ„ ØªØºÙŠÙŠØ± ØµÙŠØ§ØºØ© Ø§Ù„Ø³Ø¤Ø§Ù„.")
#

import subprocess, time
from pyngrok import ngrok


try:
    for t in ngrok.get_tunnels():
        ngrok.disconnect(t.public_url)
    ngrok.kill()
except Exception:
    pass


subprocess.run(["pkill","-f","streamlit"], check=False)
subprocess.run(["fuser","-k","8501/tcp"], check=False)

time.sleep(2)


p = subprocess.Popen(
    ["streamlit","run","yousif.py","--server.port","8501","--server.address","0.0.0.0"],
    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True
)



public_url = ngrok.connect(8501, bind_tls=True).public_url
print("Public URL:", public_url)


time.sleep(5)
for _ in range(20):
    line = p.stdout.readline()
    if not line: break
    print(line.strip())



