# -*- coding: utf-8 -*-
"""Yousif_Mohamed_Yousif_Elgendy_Law_NTI.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1g5ZE3mbPC1iBM1q8bMK4nMfL8Tc2MJ8h
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
df = pd.read_csv("/content/drive/MyDrive/Youssif/Legal data.csv")

df.head()

df.isna().sum()

df.dropna(subset=["question", "answer"], inplace=True)

df.isna().sum()

import re

def arabic_clean(text):
    text = str(text)
    text = re.sub(r'[\u0617-\u061A\u064B-\u0652]', '', text)
    text = re.sub(r'[ًٌٍَُِّْـ]', '', text)
    text = re.sub(r'[^\w\s]', '', text)
    text = re.sub(r'\d+', '', text)
    text = re.sub(r'(.)\1{2,}', r'\1', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

df['question'] = df['question'].apply(arabic_clean)
df['answer'] = df['answer'].apply(arabic_clean)

!pip install -q faiss-cpu

from transformers import AutoTokenizer, AutoModelForSeq2SeqLM
from sentence_transformers import SentenceTransformer
from sklearn.metrics.pairwise import cosine_similarity
import pandas as pd
import numpy as np
import faiss

retriever = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
questions = df['question'].tolist()
embeddings = retriever.encode(questions, show_progress_bar=True)
index = faiss.IndexFlatL2(embeddings.shape[1])
index.add(np.array(embeddings))

def retrieve_answer(query):

    query_clean = query.strip()


    matches = df[df['question'] == query_clean]

    if matches.empty:
        return None
    else:
        return matches.iloc[0]['answer']

from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("aubmindlab/aragpt2-base")
model = AutoModelForCausalLM.from_pretrained("aubmindlab/aragpt2-base")

def generate_simplified_answer(answer, max_length=120):

    prompt = f"""\
دي إجابة قانونية رسمية:
{answer}

عاوزك تشرحلي بلغة سهلة كأني واحد مش فاهم في القانون خالص، وبالعامية المصرية لو ينفع:
"""


    inputs = tokenizer.encode(prompt.strip(), return_tensors="pt", truncation=True, max_length=512)


    outputs = model.generate(
        inputs,
        max_length=max_length,
        do_sample=True,
        top_k=40,
        top_p=0.95,
        temperature=0.2,
        num_return_sequences=1,
        pad_token_id=tokenizer.eos_token_id
    )


    result = tokenizer.decode(outputs[0], skip_special_tokens=True)


    if "عاوزك" in result:
        result = result.split("عاوزك")[0].strip()
    elif "بالعامية" in result:
        result = result.split("بالعامية")[0].strip()

    return result.strip()

!pip install ddgs

from ddgs import DDGS

def search_web(query, max_results=3):
    try:
        with DDGS() as ddgs:
            results = ddgs.text(query, region='eg-en', safesearch='Moderate', max_results=max_results)
            return [result['body'] for result in results if 'body' in result]
    except Exception as e:
        print("❌ Error during search:", e)
        return []

print(search_web("ما هو الطعن بالنقض؟"))

print(search_web("كم عدد سنين حكم المؤبد "))

q = "كام سنة المؤبد"
orig = retrieve_answer(q)

if orig is None:
    print("❗ السؤال غير موجود في قاعدة البيانات القانونية.")
else:
    simp = generate_simplified_answer(orig)
    print("الإجابة الأصلية:", orig)
    print("التبسيط المقترح:", simp)

def retrieve_answer(query):
    query_clean = query.strip()
    matches = df[df['question'] == query_clean]
    if matches.empty:
        return None
    else:
        return matches.iloc[0]['answer']


q = "على من ترفع الدعوى الجزائية الخاصة؟"
orig = retrieve_answer(q)

if orig is None:
    print("❗ السؤال غير موجود في قاعدة البيانات القانونية.")
else:
    simp = generate_simplified_answer(orig)
    print("الإجابة الأصلية:", orig)
    print("التبسيط المقترح:", simp)

!pip install -q streamlit pyngrok faiss-cpu sentence-transformers transformers accelerate torch ddgs scikit-learn

# Commented out IPython magic to ensure Python compatibility.
# %%writefile yousif.py
# import streamlit as st
# import pandas as pd, re, os
# 
# 
# st.set_page_config(page_title="المساعد القانوني المبسّط", page_icon="⚖️", layout="centered")
# st.markdown("<h1 style='text-align:center'>⚖️ المساعد القانوني المبسّط</h1>", unsafe_allow_html=True)
# st.write("اكتب سؤالك القانوني وسنُرجع الإجابة الرسمية من قاعدة البيانات ثم تبسيطها بالعامية. إذا لم نجدها في القاعدة، نعرض ملخصات من الويب.")
# 
# 
# CSV_PATH = "/content/drive/MyDrive/Youssif/Legal data.csv"
# 
# 
# def arabic_clean(text):
#     text = str(text)
#     text = re.sub(r'[\u0617-\u061A\u064B-\u0652]', '', text)
#     text = re.sub(r'[ًٌٍَُِّْـ]', '', text)
#     text = re.sub(r'[^\w\s]', '', text)
#     text = re.sub(r'\d+', '', text)
#     text = re.sub(r'(.)\1{2,}', r'\1', text)
#     text = re.sub(r'\s+', ' ', text)
#     return text.strip()
# 
# 
# def load_df(path):
#     if not os.path.exists(path):
#         return None, "لا يمكن العثور على قاعدة البيانات في المسار المحدد."
#     try:
#         df = pd.read_csv(path)
#         if not {"question","answer"}.issubset(df.columns):
#             return None, "الملف لا يحتوي الأعمدة المطلوبة (question, answer)."
#         df = df.dropna(subset=["question","answer"]).copy()
#         df["question"] = df["question"].apply(arabic_clean)
#         df["answer"]   = df["answer"].apply(arabic_clean)
#         return df, None
#     except Exception:
#         return None, "تعذر قراءة الملف. تأكد من الترميز/التنسيق."
# 
# df, df_err = load_df(CSV_PATH)
# 
# 
# def retrieve_answer(query: str):
#     if df is None:
#         return None
#     q = arabic_clean(query)
#     m = df[df["question"] == q]
#     return None if m.empty else m.iloc[0]["answer"]
# 
# 
# from transformers import AutoTokenizer, AutoModelForCausalLM
# tokenizer = AutoTokenizer.from_pretrained("aubmindlab/aragpt2-base")
# model     = AutoModelForCausalLM.from_pretrained("aubmindlab/aragpt2-base")
# 
# def generate_simplified_answer(answer, max_length=256):
#     prompt = f"""\
# دي إجابة قانونية رسمية:
# {answer}
# 
# عاوزك تشرحلي بلغة سهلة كأني واحد مش فاهم في القانون خالص، وبالعامية المصرية لو ينفع:
# """
#     inputs = tokenizer.encode(prompt.strip(), return_tensors="pt", truncation=True, max_length=512)
#     outputs = model.generate(
#         inputs, max_length=max_length, do_sample=True,
#         top_k=40, top_p=0.95, temperature=0.2,
#         num_return_sequences=1, pad_token_id=tokenizer.eos_token_id
#     )
#     result = tokenizer.decode(outputs[0], skip_special_tokens=True)
#     if "عاوزك" in result:   result = result.split("عاوزك")[0].strip()
#     elif "بالعامية" in result: result = result.split("بالعامية")[0].strip()
#     return result.strip()
# 
# 
# from ddgs import DDGS
# def search_web(query, max_results=3):
#     try:
#         with DDGS() as d:
#             res = d.text(query, region="eg-ar", safesearch="Moderate", max_results=max_results)
#             return [r.get("body","") for r in res if r.get("body")]
#     except Exception:
#         return []
# 
# 
# user_q = st.text_area("✍️ اكتب سؤالك القانوني هنا:", value="", height=120, placeholder="")
# 
# if df_err:
#     st.error(df_err)
# 
# if st.button("🔎 بحث") and user_q.strip():
#     if df_err:
# 
#         web = search_web(user_q, max_results=3)
#         if web:
#             st.subheader("🌐 ملخصات من الويب:")
#             for i, w in enumerate(web, 1):
#                 st.markdown(f"**نتيجة {i}:** {w}")
#         else:
#             st.warning("لا توجد نتائج مناسبة من الويب الآن.")
#     else:
# 
#         ans = retrieve_answer(user_q)
#         if ans:
#             st.success("✅ تم العثور على إجابة في قاعدة البيانات.")
#             st.subheader("📄 الإجابة الرسمية:")
#             st.write(ans)
# 
#             st.subheader("💬 الشرح المبسّط (عامية مصرية):")
#             simp = generate_simplified_answer(ans, max_length=256)
#             st.write(simp if simp else "—")
#         else:
# 
#             st.warning("لم نجد السؤال في قاعدة البيانات. نعرض نتائج من الويب:")
#             web = search_web(user_q, max_results=3)
#             if web:
#                 st.subheader("🌐 ملخصات من الويب:")
#                 for i, w in enumerate(web, 1):
#                     st.markdown(f"**نتيجة {i}:** {w}")
#             else:
#                 st.info("لا توجد نتائج مناسبة من الويب الآن. حاول تغيير صياغة السؤال.")
#

import subprocess, time
from pyngrok import ngrok


try:
    for t in ngrok.get_tunnels():
        ngrok.disconnect(t.public_url)
    ngrok.kill()
except Exception:
    pass


subprocess.run(["pkill","-f","streamlit"], check=False)
subprocess.run(["fuser","-k","8501/tcp"], check=False)

time.sleep(2)


p = subprocess.Popen(
    ["streamlit","run","yousif.py","--server.port","8501","--server.address","0.0.0.0"],
    stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True
)



public_url = ngrok.connect(8501, bind_tls=True).public_url
print("Public URL:", public_url)


time.sleep(5)
for _ in range(20):
    line = p.stdout.readline()
    if not line: break
    print(line.strip())



